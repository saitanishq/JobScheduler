# -*- coding: utf-8 -*-
"""Copy of Google Articles Brainstorming 10 (Google Colab Files).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZEDW4pNFlAhVQOO-CRse8aoS8SYDvVc
"""

# importing libraries

import googlesearch as g # use to search keywords in google
from googlesearch import search
import time
import random
import requests
from urllib.parse import urlparse # use to parse out the domain of the url links
from datetime import datetime # use to get date and time
from IPython.display import FileLink, display

# putting the keywords in a list of strings
# each keyword with capitalization format so that it can later be used as search term as well as title of txt file

keywords = ["Corporate Health", "Top News", "Menstrual Health", "Athletes", "Research"]

# function to check if the txt file is empty or already populated with list of article links
# input: string title to locate the correct txt file
# output: boolean True if the txt file already has list, False if txt file is empty

def checkFile(title):
  with open(title + ' Article Links.txt', 'r') as f: # must already have txt file in location with name keyword + article links
    first_char = f.read(1) # get the first character of the file
    f.close()
    if first_char:
      return True # if a character is present, then the file is not empty
    else:
      return False # if a character is not present, then the file is empty

# function to take in the current list of articles and create a list of unique domains from the urls
# input: list urls which is the list of urls to be parsed
# output: set unique_domains which is a set of the unique domains, no repeats

def generateDomains(urls):
  unique_domains = set()
  for i in urls: # already know the links are unique, checked for uniqueness when added originally
    domain = urlparse(i).netloc # find just the domain of the link
    unique_domains.add(domain) # already know that the domains are unique, checked for uniqueness when added originally
  return unique_domains

# function to add new links from new list of articles to the old list of articles, ensuring unique domains
# input: list url_original which is the new list of articles
  # list url_current which is the old list of articles
  # set unique_domains which is the set of unique domains from the old list of articles
# output: list url_final which is the final list of articles
  # after only new articles with unique domains have been added to the old list

def addDomainsAndUpdateList(url_original, url_current, unique_domains):
  url_final = url_current # add the current list of articles to the final list
  for i in url_original: # iterating over the newly found links
    if i not in url_final: # check that the newly found links are not already in the current list before adding
      domain = urlparse(i).netloc # find just the domain of the link
      if domain not in unique_domains: # check that the newly found links are not the same domain as another in the current list
        unique_domains.add(domain) # update the current list of unique domains so the new links do not overlap on each other
        url_final.append(i) # can finally add the link if not already added and if the domain is unique
  return url_final

# function to take in the current list of articles, add new links if not already listed
# input: string keyword, list url_old which is the current list of articles in the txt file
# output: list url_all which is the updated list of old and new articles, no repeats

def addLinks_test(keyword, url_old):
  url_new = list(g.search("women's health " + keyword, stop = 10, lang = 'en')) # search for articles, put links in a list
  domains_current = generateDomains(url_old) # make a list of the unique domains that are in the current list of links
  url_all = addDomainsAndUpdateList(url_new, url_old, domains_current) # check that links are not already in the list and have unique domains
  random.shuffle(url_all) # shuffle the list of articles so not in any order, get more variety
  return url_all

def addLinks(keyword, url_old):
    query = "women's health " + keyword
    url_new = []

    # Perform the search and limit the results manually
    for url in search(query, num_results=10, lang='en'):
        url_new.append(url)

    domains_current = generateDomains(url_old)  # Generate a list of unique domains
    url_all = addDomainsAndUpdateList(url_new, url_old, domains_current)  # Update the list with unique domains
    random.shuffle(url_all)  # Shuffle the list for variety
    return url_all

# function to get the current date and time
# input: none
# output: list dt_list giving information as [date, time]

def getDataAndTime():
  now = datetime.now() # get now using datetime
  dt = now.strftime("%d/%m/%Y %H:%M:%S") # date and time given as dd/mm/YY H:M:S
  dt_string = str(dt) # reformat as a string
  dt_list = list(dt_string.split()) # separating out date and time
  return dt_list

# function to get the length of the list of articles
# input: list urls
# output: string total which is the numeric length of the list

def getLength(urls):
  total = str(len(urls))
  return total

# function to convert the list of urls to a string format so it can be added to the txt file
# input: list url_list
# output: string url_string of the original list in string format

def listToString(url_list):
  url_string = ""
  for i in url_list:
      url_string += i + ", " # separating links with ", " so it still looks like a list
  url_string = url_string[:-2] # getting rid of the final ", "
  return url_string

# function to clear out current txt file and put in the new date, time, length of the list, and updated links
# input: string title to locate the txt file
  # string date which is the current date
  # string time which is the current time
  # string length which gives the number of articles in the list
  # string content which is the updated links to populate the txt file
# output: none, txt files should be populated with new content

def writeFile(title, date, time, length, content):
  with open(title + ' Article Links.txt', 'w') as f:
    f.truncate(0) # clear out current contents of the file
    f.write(date) # add in the date
    f.write(time) # add in the time
    f.write(length) # add in the length of the list of articles
    f.write(content) # add in the updated list of links as a string
    f.close()

# function to download the txt files
# input: string title to locate the txt file
# output: none, txt files should be downloaded

def downloadFile(title):
    # Create a download link for the file in Jupyter Notebook
    display(FileLink(f'{title} Article Links.txt'))

# function to read txt files, store the contents, and convert from a string to list format so the list can be updated
# input: string title to locate the correct txt file
# output: list url_current which is list of urls contained in the txt file

def readFile(title):
  with open(title + ' Article Links.txt', 'r') as f:
    next(f) # skip first line which is the date
    next(f) # skip second line which is the time
    next(f) # skip third line which is the length of the list
    url_read = f.read() # begin reading 2 lines down, url_read is a string
    f.close()
  url_current = list(url_read.split(", ")) # convert content from a string to a list
  return url_current

# function to edit the txt files and add new article links
# input: string topic to be used in keyword google search
# output: none, txt fils should be updated

def replaceFile(topic):
  if not checkFile(topic): # check if the file is entirely empty
    url_old = [] # if the file is empty, then the current list of articles is empty
  else: # if the file is not empty, it will have a current list of articles to be read and updated
    url_old = readFile(topic) # read the current list of articles
  print(url_old)
  url_list = addLinks(topic, url_old) # add links to the the curerent list of articles in list form
  print(url_list)
  date = "Date: " + str(getDataAndTime()[0]) + "\n" # reformatting date by itself
  time = "Time: " + str(getDataAndTime()[1]) + "\n" # reformatting time by itself
  length = length = "Length: " + getLength(url_list) + " Articles" + "\n" # reformatting the length by itself
  print(length)
  url_string = listToString(url_list) # change the list to string form compatible with txt file
  writeFile(topic, date, time, length, url_string) # add new contents to the files
  downloadFile(topic) # implement if want to download the files

# populating the txt files links

for i in keywords: # populate a txt file for each of the keywords
  replaceFile(i) # replace the file with a file that has an updated list of articles
